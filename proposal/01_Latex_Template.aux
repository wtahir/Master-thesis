\relax 
\babel@aux{english}{}
\citation{martins2009crater}
\citation{koeberl1994african}
\citation{ivanov2002comparison}
\citation{opik1965mariner}
\citation{hartmann1981chronology}
\citation{soderblom1970distribution}
\citation{williams2018dating}
\citation{ivanov2002comparison}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\citation{robinson2010lunar}
\citation{robinson2010lunar}
\citation{sawabe_2006}
\citation{sawabe_2006}
\citation{krizhevsky_imagenet_2012}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bounding Box Proposal}{5}}
\citation{redmon_you_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of two images which are represented as vectors and pixelwise difference is calculated to compare both images with L1 distance. This example shows one color channel. All the pixel-wise differences are added to denote a single digit value. If this value is close to zero then it indicates that images are identical. A large value shows that both images are very different.}}{6}}
\newlabel{fig: Example of two images which are represented as vectors and pixel-wise difference is calculated to compare both images with L1 distance. This example shows one color channel. All the pixel-wise differences are added to denote a single digit value. If this value is close to zero then it indicates that images are identical. A large value shows that both images are very different.}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linear Classification}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The graphical representation of eq ($f(x_i, W, b) = Wx_i + b$). It shows that an image $x_i$ is represented as a one dimensional vector which is multiplied by set weights and resultant is added with a bias leading to the highest score for detection of cat image. The weights are set badly as classifier thinks it is a dog instead of a cat.}}{8}}
\newlabel{fig: linear classifier}{{2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Normalization}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Loss Function}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Cross Entropy Loss}{9}}
\newlabel{crossent}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Hinge Loss}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Activation Functions}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Sigmoid Function}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sigmoid Function}}{11}}
\newlabel{fig: Sigmoid}{{3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}ReLU Function}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ReLU Function}}{12}}
\newlabel{fig: relu}{{4}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Softmax Classifier}{12}}
\newlabel{singlesoftmax}{{6}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Neural Network Overview}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A typical Neural Network architecture}}{13}}
\newlabel{fig: Neural Network architecture}{{5}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Convolutional Neural Network Overview}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Concept of layers in CNNs}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Convolutional layer}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Convolutional Operation}}{14}}
\newlabel{fig:Convolutional Operation}{{6}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Pooling layer}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Pooling Operation}}{15}}
\newlabel{fig:Pooling Operation}{{7}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3}Fully Connected layer}{15}}
\citation{2016face}
\citation{hiesinger2000ages}
\citation{stoffler2001stratigraphy}
\citation{stoffler2001stratigraphy}
\citation{hiesinger2000ages}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Convolutional Neural Network Architecture \cite  {2016face}}}{16}}
\newlabel{fig: CNN architecture}{{8}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Lunar Production Function}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Hartmann Production Function (HPF)}{16}}
\citation{ivanov2002comparison}
\citation{neukum1994crater}
\citation{neukum1983meteoritenbombardement}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Neukum Production Function (NPF)}{17}}
\citation{sawabe_2006}
\citation{sawabe_2006}
\citation{neukum1975cratering}
\citation{martins2009crater}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces (a) The incremental representation of the Hartmann pro- duction function (HPF). The HPF, in a direct sense, is the set of points shown in the plot. Straight lines represent the piece-wise power law fitting to the data (equation (1)). (b) Comparison of pro- duction functions derived by Hartmann (HPF) and Neukum (NPF) in the R plot representation. The maximum discrepancy between HPF (2) and NPF (3) (roughly a factor of 3) is observed in the diameter bins around D ~ 6 km. Below D ~ 1 km and in the diam- eter range of 30\IeC {\textendash }100 km, the HPF and NPF give the same or simi- lar results. Fitting the HPF to equation (3), we obtain a model age of 3.4 G.y. The NPF, which is fit to the wide range count of impact craters in the Orientale Basin, yields a model age of ~3.7 G.y. The dashed line 1 represents the approximate saturation level estimated by Hartmann (1995).}}{19}}
\newlabel{HPF}{{9}{19}}
\citation{girshick_rich_2013}
\citation{he_mask_2017}
\citation{he_mask_2017}
\citation{redmon_you_2016}
\@writefile{toc}{\contentsline {section}{\numberline {5}Related Work}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Methodology}{20}}
\bibstyle{apalike}
\bibdata{mybib}
\bibcite{2016face}{El~Khiyari and Wechsler, 2016}
\bibcite{girshick_rich_2013}{Girshick et~al., }
\bibcite{hartmann1981chronology}{Hartmann, 1981}
\bibcite{he_mask_2017}{He et~al., }
\bibcite{hiesinger2000ages}{Hiesinger et~al., 2000}
\bibcite{ivanov2002comparison}{Ivanov, 2002}
\bibcite{koeberl1994african}{Koeberl, 1994}
\bibcite{krizhevsky_imagenet_2012}{Krizhevsky et~al., 2012}
\bibcite{martins2009crater}{Martins et~al., 2009}
\bibcite{neukum1983meteoritenbombardement}{Neukum, 1983}
\bibcite{neukum1994crater}{Neukum and Ivanov, 1994}
\bibcite{neukum1975cratering}{Neukum et~al., 1975}
\bibcite{opik1965mariner}{Opik, 1965}
\bibcite{redmon_you_2016}{Redmon et~al., }
\bibcite{robinson2010lunar}{Robinson et~al., 2010}
\bibcite{sawabe_2006}{Sawabe et~al., 2006}
\@writefile{toc}{\contentsline {section}{References}{22}}
\bibcite{soderblom1970distribution}{Soderblom, 1970}
\bibcite{stoffler2001stratigraphy}{St{\"o}ffler and Ryder, 2001}
\bibcite{williams2018dating}{Williams et~al., 2018}
