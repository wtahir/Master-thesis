\relax 
\babel@aux{english}{}
\citation{martins2009crater}
\citation{koeberl1994african}
\citation{ivanov2002comparison}
\citation{opik1965mariner}
\citation{hartmann1981chronology}
\citation{soderblom1970distribution}
\citation{williams2018dating}
\citation{ivanov2002comparison}
\citation{robinson2010lunar}
\citation{robinson2010lunar}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\citation{sawabe_2006}
\citation{sawabe_2006}
\citation{stepinski2012detecting}
\citation{greeley1970precision}
\citation{krizhevsky_imagenet_2012}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Statement}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Research Approach}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Background}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Bounding Box Proposal}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of two images which are represented as vectors and pixelwise difference is calculated to compare both images with L1 distance. This example shows one color channel. All the pixel-wise differences are added to denote a single digit value. If this value is close to zero then it indicates that images are identical. A large value shows that both images are very different.}}{6}}
\newlabel{fig: Example of two images which are represented as vectors and pixel-wise difference is calculated to compare both images with L1 distance. This example shows one color channel. All the pixel-wise differences are added to denote a single digit value. If this value is close to zero then it indicates that images are identical. A large value shows that both images are very different.}{{1}{6}}
\citation{redmon_you_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Linear Classification}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The graphical representation of eq ($f(x_i, W, b) = Wx_i + b$). It shows that an image $x_i$ is represented as a one dimensional vector which is multiplied by set weights and resultant is added with a bias leading to the highest score for detection of cat image. The weights are set badly as classifier thinks it is a dog instead of a cat.}}{8}}
\newlabel{fig: linear classifier}{{2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Normalization}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Loss Function}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Cross Entropy Loss}{9}}
\newlabel{crossent}{{1}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Activation Functions}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sigmoid Function}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sigmoid Function}}{11}}
\newlabel{fig: Sigmoid}{{3}{11}}
\citation{krizhevsky_imagenet_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ReLU Function}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ReLU Function}}{12}}
\newlabel{fig: relu}{{4}{12}}
\citation{krizhevsky2012imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A CNN of four layers trained with ReLU in solid line and tanh in dashed line. It shows that ReLU reaches a 25\% training rate on CIFAR-10 six times faster as compared to tanh.}}{13}}
\newlabel{fig: relu_fast}{{5}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Softmax Classifier}{13}}
\newlabel{singlesoftmax}{{6}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Neural Network Overview}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A typical Neural Network architecture}}{15}}
\newlabel{fig: Neural Network architecture}{{6}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Convolutional Neural Network Overview}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Concept of layers in CNNs}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}Convolutional layer}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Convolutional Operation}}{16}}
\newlabel{fig:Convolutional Operation}{{7}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}Pooling layer}{16}}
\citation{2016face}
\citation{hiesinger2000ages}
\citation{stoffler2001stratigraphy}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Pooling Operation}}{17}}
\newlabel{fig:Pooling Operation}{{8}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}Fully Connected layer}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Convolutional Neural Network Architecture \cite  {2016face}}}{17}}
\newlabel{fig: CNN architecture}{{9}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Lunar Production Function}{17}}
\citation{stoffler2001stratigraphy}
\citation{hiesinger2000ages}
\citation{ivanov2002comparison}
\citation{neukum1994crater}
\citation{neukum1983meteoritenbombardement}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Hartmann Production Function (HPF)}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}Neukum Production Function (NPF)}{19}}
\citation{sawabe_2006}
\citation{sawabe_2006}
\citation{neukum1975cratering}
\citation{martins2009crater}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces (a) The incremental representation of the Hartmann pro- duction function (HPF). The HPF, in a direct sense, is the set of points shown in the plot. Straight lines represent the piece-wise power law fitting to the data (equation (1)). (b) Comparison of pro- duction functions derived by Hartmann (HPF) and Neukum (NPF) in the R plot representation. The maximum discrepancy between HPF (2) and NPF (3) (roughly a factor of 3) is observed in the diameter bins around D ~ 6 km. Below D ~ 1 km and in the diam- eter range of 30\IeC {\textendash }100 km, the HPF and NPF give the same or simi- lar results. Fitting the HPF to equation (3), we obtain a model age of 3.4 G.y. The NPF, which is fit to the wide range count of impact craters in the Orientale Basin, yields a model age of ~3.7 G.y. The dashed line 1 represents the approximate saturation level estimated by Hartmann (1995).}}{20}}
\newlabel{HPF}{{10}{20}}
\citation{long2015fully}
\citation{silburt2019lunar}
\citation{silburt2019lunar}
\@writefile{toc}{\contentsline {section}{\numberline {7}Related Work}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Most left is the Moon DEM sample and middle image shows prediction of craters and most right one shows the missing classifications which are marked in red circles}}{22}}
\newlabel{dem}{{11}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Model Architecture}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces This is example of neuronal structures segmentation. It shows the overlap-tile strategy for seamless segmentation of large images. The yellow area is target for prediction and data inside the blue area is required as in input for prediction. Missing input data is exptrapolated by mirroring}}{23}}
\newlabel{fig:tiling}{{12}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces U-net Architecture}}{24}}
\newlabel{fig:U-net}{{13}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Methodology}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Image Annotation}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Example of cropped images and corresponding labeled images. Annotation is performed using VIA tool}}{25}}
\newlabel{ann}{{14}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Pre-processing}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Visualization of binary masks after projection from annotated (json format) images that belong to Figure 13}}{26}}
\newlabel{mask}{{15}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Data Augmentation}{26}}
\citation{ronneberger2015u}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Contrast Limited Adaptive Histogram Equalization (CLAHE)}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Histogram of an image before contrast limited adaptive histogram equilization}}{27}}
\newlabel{hist}{{16}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Histogram of an image after contrast limited adaptive histogram equilization}}{27}}
\newlabel{clahe}{{17}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Training}{28}}
\bibstyle{apalike}
\bibdata{mybib}
\bibcite{2016face}{El~Khiyari and Wechsler, 2016}
\bibcite{greeley1970precision}{Greeley and Gault, 1970}
\bibcite{hartmann1981chronology}{Hartmann, 1981}
\bibcite{hiesinger2000ages}{Hiesinger et~al., 2000}
\bibcite{ivanov2002comparison}{Ivanov, 2002}
\bibcite{koeberl1994african}{Koeberl, 1994}
\bibcite{krizhevsky_imagenet_2012}{Krizhevsky et~al., 2012a}
\bibcite{krizhevsky2012imagenet}{Krizhevsky et~al., 2012b}
\bibcite{long2015fully}{Long et~al., 2015}
\bibcite{martins2009crater}{Martins et~al., 2009}
\bibcite{neukum1983meteoritenbombardement}{Neukum, 1983}
\bibcite{neukum1994crater}{Neukum and Ivanov, 1994}
\bibcite{neukum1975cratering}{Neukum et~al., 1975}
\bibcite{opik1965mariner}{Opik, 1965}
\bibcite{redmon_you_2016}{Redmon et~al., }
\bibcite{robinson2010lunar}{Robinson et~al., 2010}
\@writefile{toc}{\contentsline {section}{References}{29}}
\bibcite{ronneberger2015u}{Ronneberger et~al., 2015}
\bibcite{sawabe_2006}{Sawabe et~al., 2006}
\bibcite{silburt2019lunar}{Silburt et~al., 2019}
\bibcite{soderblom1970distribution}{Soderblom, 1970}
\bibcite{stepinski2012detecting}{Stepinski et~al., 2012}
\bibcite{stoffler2001stratigraphy}{St{\"o}ffler and Ryder, 2001}
\bibcite{williams2018dating}{Williams et~al., 2018}
