@phdthesis{soderblom1970distribution,
  title={The distribution and ages of regional lithologies in the lunar maria},
  author={Soderblom, Laurence Albert},
  year={1970},
  school={California Institute of Technology}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{li2018h,
  title={H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes},
  author={Li, Xiaomeng and Chen, Hao and Qi, Xiaojuan and Dou, Qi and Fu, Chi-Wing and Heng, Pheng-Ann},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={12},
  pages={2663--2674},
  year={2018},
  publisher={IEEE}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{neukum1983meteoritenbombardement,
  title={Meteoritenbombardement und Datierung planetarer Oberfl{\"a}chen, Habilitation dissertation for faculty membership},
  author={Neukum, G},
  journal={Ludwig-Maximilians-University, Munich},
  pages={186},
  year={1983}
}

@article{hiesinger2000ages,
  title={Ages of mare basalts on the lunar nearside},
  author={Hiesinger, Harald and Jaumann, Ralf and Neukum, Gerhard and Head III, James W},
  journal={Journal of Geophysical Research: Planets},
  volume={105},
  number={E12},
  pages={29239--29275},
  year={2000},
  publisher={Wiley Online Library}
}

@article{neukum1994crater,
  title={Crater size distributions and impact probabilities on Earth from lunar, terrestrial-planet, and asteroid cratering data},
  author={Neukum, G and Ivanov, BA},
  journal={Hazards due to Comets and Asteroids},
  volume={1},
  pages={359--416},
  year={1994}
}

@incollection{stoffler2001stratigraphy,
  title={Stratigraphy and isotope ages of lunar geologic units: Chronological standard for the inner solar system},
  author={St{\"o}ffler, Dieter and Ryder, G},
  booktitle={Chronology and evolution of Mars},
  pages={9--54},
  year={2001},
  publisher={Springer}
}

@incollection{stoffler2001stratigraphy,
  title={Stratigraphy and isotope ages of lunar geologic units: Chronological standard for the inner solar system},
  author={St{\"o}ffler, Dieter and Ryder, G},
  booktitle={Chronology and evolution of Mars},
  pages={9--54},
  year={2001},
  publisher={Springer}
}

@article{hiesinger2000ages,
  title={Ages of mare basalts on the lunar nearside},
  author={Hiesinger, Harald and Jaumann, Ralf and Neukum, Gerhard and Head III, James W},
  journal={Journal of Geophysical Research: Planets},
  volume={105},
  number={E12},
  pages={29239--29275},
  year={2000},
  publisher={Wiley Online Library}
}

@article{soderblom1972technique,
  title={Technique for rapid determination of relative ages of lunar areas from orbital photography},
  author={Soderblom, Laurence A and Lebofsky, Larry A},
  journal={Journal of Geophysical Research},
  volume={77},
  number={2},
  pages={279--296},
  year={1972},
  publisher={Wiley Online Library}
}

@article{hartmann1981chronology,
  title={Chronology of planetary volcanism by comparative studies of planetary cratering},
  author={Hartmann, William K},
  journal={Basaltic Volcanism on the Terrestrial Planets.},
  pages={1049--1127},
  year={1981},
  publisher={Pergamon}
}


@misc{opik1965mariner,
  title={Mariner IV and craters on Mars},
  author={Opik, EJ},
  year={1965}
}

@article{williams2018dating,
  title={Dating very young planetary surfaces from crater statistics: A review of issues and challenges},
  author={Williams, Jean-Pierre and van der Bogert, Carolyn H and Pathare, Asmin V and Michael, Gregory G and Kirchoff, Michelle R and Hiesinger, Harald},
  journal={Meteoritics \& Planetary Science},
  volume={53},
  number={4},
  pages={554--582},
  year={2018},
  publisher={Wiley Online Library}
}

@article{ivanov2002comparison,
  title={The comparison of size-frequency distributions of impact craters and asteroids and the planetary cratering rate},
  author={Ivanov, BA},
  journal={Asteroids III},
  volume={1},
  pages={89--101},
  year={2002},
  publisher={this volume. Univ. of Arizona, Tucson}
}


@inproceedings{neukum1975cratering,
  title={Cratering in the Earth-Moon system-Consequences for age determination by crater counting},
  author={Neukum, Gerhard and K{\"o}nig, B and Fechtig, H and Storzer, D},
  booktitle={Lunar and Planetary Science Conference Proceedings},
  volume={6},
  pages={2597--2620},
  year={1975}
}

@article{2016face,
  title={Face recognition across time lapse using convolutional neural networks},
  author={El Khiyari, Hachim and Wechsler, Harry},
  journal={Journal of Information Security},
  volume={7},
  number={03},
  pages={141},
  year={2016},
  publisher={Scientific Research Publishing}
}

@InProceedings{Redmon_2016_CVPR,
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
title = {You Only Look Once: Unified, Real-Time Object Detection},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{martins2009crater,
  title={Crater detection by a boosting approach},
  author={Martins, Ricardo and Pina, Pedro and Marques, Jorge S and Silveira, Margarida},
  journal={IEEE Geoscience and Remote Sensing Letters},
  volume={6},
  number={1},
  pages={127--131},
  year={2009},
  publisher={IEEE}
}
@article{koeberl1994african,
  title={African meteorite impact craters: Characteristics and geological importance},
  author={Koeberl, Christian},
  journal={Journal of African Earth Sciences},
  volume={18},
  number={4},
  pages={263--295},
  year={1994},
  publisher={Elsevier}
}
@article{girshick_rich_2013,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical {PASCAL} {VOC} dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision ({mAP}) by more than 30\% relative to the previous best result on {VOC} 2012---achieving a {mAP} of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks ({CNNs}) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with {CNNs}, we call our method R-{CNN}: Regions with {CNN} features. We also compare R-{CNN} to {OverFeat}, a recently proposed sliding-window detector based on a similar {CNN} architecture. We find that R-{CNN} outperforms {OverFeat} by a large margin on the 200-class {ILSVRC}2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	journaltitle = {{arXiv}:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	urldate = {2018-03-27},
	date = {2013-11-11},
	eprinttype = {arxiv},
	eprint = {1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1311.2524 PDF:/Users/wtahir/Zotero/storage/GTLDGBHR/Girshick et al. - 2013 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:/Users/wtahir/Zotero/storage/GY4FNAHE/1311.html:text/html}
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC}14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC}14 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	pages = {1--9},
	publisher = {{IEEE}},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2018-04-09},
	date = {2015-06},
	langid = {english},
	file = {Szegedy et al. - 2015 - Going deeper with convolutions.pdf:/Users/wtahir/Zotero/storage/ZH2A4D2P/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2018-04-09},
	date = {2015-05},
	langid = {english},
	file = {CNN based edge detection.pdf:/Users/wtahir/Zotero/storage/4F8WS92L/CNN based edge detection.pdf:application/pdf;LeCun et al. - 2015 - Deep learning.pdf:/Users/wtahir/Zotero/storage/243SXBTA/LeCun et al. - 2015 - Deep learning.pdf:application/pdf}
}

@inproceedings{chen_vehicle_2013,
	title = {Vehicle Detection in Satellite Images by Parallel Deep Convolutional Neural Networks},
	isbn = {978-1-4799-2190-4},
	url = {http://ieeexplore.ieee.org/document/6778306/},
	doi = {10.1109/ACPR.2013.33},
	abstract = {Deep convolutional Neural Networks ({DNN}) is the state-of-the-art machine learning method. It has been used in many recognition tasks including handwritten digits, Chinese words and trafﬁc signs, etc. However, training and test {DNN} are time-consuming tasks. In practical vehicle detection application, both speed and accuracy are required. So increasing the speeds of {DNN} while keeping its high accuracy has signiﬁcant meaning for many recognition and detection applications. We introduce parallel branches into the {DNN}. The maps of the layers of {DNN} are divided into several parallel branches, each branch has the same number of maps. There are not direct connections between different branches. Our parallel {DNN} ({PNN}) keeps the same structure and dimensions of the {DNN}, reducing the total number of connections between maps. The more number of branches we divide, the more swift the speed of the {PNN} is, the conventional {DNN} becomes a special form of {PNN} which has only one branch. Experiments on large vehicle database showed that the detection accuracy of {PNN} dropped slightly with the speed increasing. Even the fastest {PNN} (10 times faster than {DNN}), whose branch has only two maps, fully outperformed the traditional methods based on features (such as {HOG}, {LBP}). In fact, {PNN} provides a good solution way for compromising the speed and accuracy requirements in many applications.},
	pages = {181--185},
	publisher = {{IEEE}},
	author = {Chen, Xueyun and Xiang, Shiming and Liu, Cheng-Lin and Pan, Chun-Hong},
	urldate = {2018-04-09},
	date = {2013-11},
	langid = {english},
	file = {Chen et al. - 2013 - Vehicle Detection in Satellite Images by Parallel .pdf:/Users/wtahir/Zotero/storage/7GRF7YWX/Chen et al. - 2013 - Vehicle Detection in Satellite Images by Parallel .pdf:application/pdf}
}

@article{zhong_satcnn:_2017,
	title = {{SatCNN}: satellite image dataset classification using agile convolutional neural networks},
	volume = {8},
	issn = {2150-704X, 2150-7058},
	url = {https://www.tandfonline.com/doi/full/10.1080/2150704X.2016.1235299},
	doi = {10.1080/2150704X.2016.1235299},
	shorttitle = {{SatCNN}},
	abstract = {With the launch of various remote-sensing satellites, more and more high-spatial resolution remote-sensing ({HSR}-{RS}) images are becoming available. Scene classiﬁcation of such a huge volume of {HSR}-{RS} images is a big challenge for the eﬃciency of the feature learning and model training. The deep convolutional neural network ({CNN}), a typical deep learning model, is an eﬃcient end-toend deep hierarchical feature learning model that can capture the intrinsic features of input {HSR}-{RS} images. However, most published {CNN} architectures are borrowed from natural scene classiﬁcation with thousands of training samples, and they are not designed for {HSR}-{RS} images. In this paper, we propose an agile {CNN} architecture, named as {SatCNN}, for {HSR}-{RS} image scene classiﬁcation. Based on recent improvements to modern {CNN} architectures, we use more eﬃcient convolutional layers with smaller kernels to build an eﬀective {CNN} architecture. Experiments on {SAT} data sets conﬁrmed that {SatCNN} can quickly and eﬀectively learn robust features to handle the intra-class diversity even with small convolutional kernels, and the deeper convolutional layers allow spontaneous modelling of the relative spatial relationships. With the help of fast graphics processing unit acceleration, {SatCNN} can be trained within about 40 min, achieving overall accuracies of 99.65\% and 99.54\%, which is the state-ofthe-art for {SAT} data sets.},
	pages = {136--145},
	number = {2},
	journaltitle = {Remote Sensing Letters},
	author = {Zhong, Yanfei and Fei, Feng and Liu, Yanfei and Zhao, Bei and Jiao, Hongzan and Zhang, Liangpei},
	urldate = {2018-04-09},
	date = {2017-02},
	langid = {english},
	file = {Zhong et al. - 2017 - SatCNN satellite image dataset classification usi.pdf:/Users/wtahir/Zotero/storage/TU2SLCEY/Zhong et al. - 2017 - SatCNN satellite image dataset classification usi.pdf:application/pdf}
}

@article{martins2009crater,
  title={Crater detection by a boosting approach},
  author={Martins, Ricardo and Pina, Pedro and Marques, Jorge S and Silveira, Margarida},
  journal={IEEE Geoscience and Remote Sensing Letters},
  volume={6},
  number={1},
  pages={127--131},
  year={2009},
  publisher={IEEE}
}

@article{robinson2010lunar,
  title={Lunar reconnaissance orbiter camera (LROC) instrument overview},
  author={Robinson, MS and Brylow, SM and Tschimmel, M and Humm, D and Lawrence, SJ and Thomas, PC and Denevi, BW and Bowman-Cisneros, E and Zerr, J and Ravine, MA and others},
  journal={Space science reviews},
  volume={150},
  number={1-4},
  pages={81--124},
  year={2010},
  publisher={Springer}
}

@article{zhang_s-cnn-based_2016,
	title = {S-{CNN}-{BASED} {SHIP} {DETECTION} {FROM} {HIGH}-{RESOLUTION} {REMOTE} {SENSING} {IMAGES}},
	volume = {{XLI}-B7},
	issn = {2194-9034},
	url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B7/423/2016/isprs-archives-XLI-B7-423-2016.pdf},
	doi = {10.5194/isprsarchives-XLI-B7-423-2016},
	abstract = {Reliable ship detection plays an important role in both military and civil ﬁelds. However, it makes the task difﬁcult with high-resolution remote sensing images with complex background and various types of ships with different poses, shapes and scales. Related works mostly used gray and shape features to detect ships, which obtain results with poor robustness and efﬁciency. To detect ships more automatically and robustly, we propose a novel ship detection method based on the convolutional neural networks ({CNNs}), called {SCNN}, fed with speciﬁcally designed proposals extracted from the ship model combined with an improved saliency detection method. Firstly we creatively propose two ship models, the “V” ship head model and the “{\textbar}{\textbar}” ship body one, to localize the ship proposals from the line segments extracted from a test image. Next, for offshore ships with relatively small sizes, which cannot be efﬁciently picked out by the ship models due to the lack of reliable line segments, we propose an improved saliency detection method to ﬁnd these proposals. Therefore, these two kinds of ship proposals are fed to the trained {CNN} for robust and efﬁcient detection. Experimental results on a large amount of representative remote sensing images with different kinds of ships with varied poses, shapes and scales demonstrate the efﬁciency and robustness of our proposed S-{CNN}-Based ship detector.},
	pages = {423--430},
	journaltitle = {{ISPRS} - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Zhang, Ruiqian and Yao, Jian and Zhang, Kao and Feng, Chen and Zhang, Jiadong},
	urldate = {2018-04-09},
	date = {2016-06-21},
	langid = {english},
	file = {Zhang et al. - 2016 - S-CNN-BASED SHIP DETECTION FROM HIGH-RESOLUTION RE.pdf:/Users/wtahir/Zotero/storage/IEDTTJ3I/Zhang et al. - 2016 - S-CNN-BASED SHIP DETECTION FROM HIGH-RESOLUTION RE.pdf:application/pdf}
}

@inproceedings{ishii_surface_2015,
	title = {Surface object recognition with {CNN} and {SVM} in Landsat 8 images},
	isbn = {978-4-901122-14-6},
	url = {http://ieeexplore.ieee.org/document/7153200/},
	doi = {10.1109/MVA.2015.7153200},
	abstract = {There is a series of earth observation satellites called Landsat, which send a very large amount of image data every day such that it is hard to analyze manually. Thus an eﬀective application of machine learning techniques to automatically analyze such data is called for. In surface object recognition, which is one of the important applications of such data, the distribution of a speciﬁc object on the surface is surveyed. In this paper, we propose and compare two methods for surface object recognition, one using the convolutional neural network ({CNN}) and the other support vector machine ({SVM}). In our experiments, {CNN} showed higher performance than {SVM}. In addition, we observed that the number of negative samples have a inﬂuence on the performance, and it is necessary to select the number of them for practical use.},
	pages = {341--344},
	publisher = {{IEEE}},
	author = {Ishii, Tomohiro and Nakamura, Ryosuke and Nakada, Hidemoto and Mochizuki, Yoshihiko and Ishikawa, Hiroshi},
	urldate = {2018-04-09},
	date = {2015-05},
	langid = {english},
	file = {Ishii et al. - 2015 - Surface object recognition with CNN and SVM in Lan.pdf:/Users/wtahir/Zotero/storage/SEVX5QPX/Ishii et al. - 2015 - Surface object recognition with CNN and SVM in Lan.pdf:application/pdf}
}

@article{krizhevsky_imagenet_2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient {GPU} implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2018-04-23},
	year = {2012},
	langid = {english},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/Users/wtahir/Zotero/storage/L5K2Z3PT/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@inproceedings{xie_aggregated_2017,
	title = {Aggregated residual transformations for deep neural networks},
	isbn = {1-5386-0457-4},
	eventtitle = {Computer Vision and Pattern Recognition ({CVPR}), 2017 {IEEE} Conference on},
	pages = {5987--5995},
	publisher = {{IEEE}},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	date = {2017}
}

@article{he_mask_2017,
	title = {Mask R-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-{CNN}, extends Faster R-{CNN} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-{CNN} is simple to train and adds only a small overhead to Faster R-{CNN}, running at 5 fps. Moreover, Mask R-{CNN} is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the {COCO} suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-{CNN} outperforms all existing, single-model entries on every task, including the {COCO} 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	journaltitle = {{arXiv}:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	urldate = {2018-05-13},
	date = {2017-03-20},
	eprinttype = {arxiv},
	eprint = {1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1703.06870 PDF:/Users/wtahir/Zotero/storage/X5K5UUZ7/He et al. - 2017 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/wtahir/Zotero/storage/3N3ZU6WI/1703.html:text/html}
}

@article{sawabe_2006,
	title = {Automated detection and classification of lunar craters using multiple approaches},
	volume = {37},
	issn = {02731177},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0273117705010392},
	doi = {10.1016/j.asr.2005.08.022},
	abstract = {Many missions such as Clementine and {SELENE} ({SELenological} and Engineering Explorer) take lunar images for examination. A large volume of imagery data has already been archived and much more is on the way. Extracting the necessary information from the already large and ever growing volume of data is the crucial problem that needs to be overcome.},
	pages = {21--27},
	number = {1},
	journaltitle = {Advances in Space Research},
	author = {Sawabe, Y. and Matsunaga, T. and Rokugawa, S.},
	urldate = {2018-05-14},
	year = {2006},
	langid = {english},
	file = {Sawabe et al. - 2006 - Automated detection and classification of lunar cr.pdf:/Users/wtahir/Zotero/storage/ZMHMWC8I/Sawabe et al. - 2006 - Automated detection and classification of lunar cr.pdf:application/pdf}
}

@article{chartock_extraction_????,
	title = {Extraction of Building Footprints from Satellite Imagery},
	abstract = {We use a Fully Convolutional Neural Network to extract bounding polygons for building footprints. Our network takes in 11-band satellite image data and produces signed distance labels, denoting which pixels are inside and outside of building footprints. Finally, we post-process the data to produce bounding polygons. When a similar dataset was ﬁrst released as part of the ﬁrst {SpaceNet} Challenge, the winning implementation produced an F1 score of 0.25 and used no deep learning; our approach outperforms this with an F1 score of 0.34.},
	pages = {8},
	author = {Chartock, Elliott and {LaRow}, Whitney and Singh, Vijay},
	langid = {english},
	file = {Chartock et al. - Extraction of Building Footprints from Satellite I.pdf:/Users/wtahir/Zotero/storage/IZRX525G/Chartock et al. - Extraction of Building Footprints from Satellite I.pdf:application/pdf}
}

@inproceedings{redmon_you_2016,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	pages = {779--788},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2018-05-14},
	date = {2016-06},
	langid = {english},
	file = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:/Users/wtahir/Zotero/storage/ECZHW82K/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}
