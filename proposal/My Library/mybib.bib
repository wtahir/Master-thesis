@article{neukum1983meteoritenbombardement,
  title={Meteoritenbombardement und datierung planetarer oberflachen},
  author={Neukum, G},
  journal={Habilitation Dissertation for Faculty Membership, Ludwig-Maximilians-Univ.},
  year={1983}
}

@article{ivanov2002comparison,
  title={The comparison of size-frequency distributions of impact craters and asteroids and the planetary cratering rate},
  author={Ivanov, BA},
  journal={Asteroids III},
  volume={1},
  pages={89--101},
  year={2002},
  publisher={this volume. Univ. of Arizona, Tucson}
}


@article{he_mask_2017,
	title = {Mask R-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-{CNN}, extends Faster R-{CNN} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-{CNN} is simple to train and adds only a small overhead to Faster R-{CNN}, running at 5 fps. Moreover, Mask R-{CNN} is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the {COCO} suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-{CNN} outperforms all existing, single-model entries on every task, including the {COCO} 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	journaltitle = {{arXiv}:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	urldate = {2018-05-13},
	date = {2017-03-20},
	eprinttype = {arxiv},
	eprint = {1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{girshick_rich_2013,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2018-03-27},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {arXiv\:1311.2524 PDF:files/4/Girshick et al. - 2013 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:files/5/1311.html:text/html}
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	language = {en},
	urldate = {2018-04-09},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
	file = {Szegedy et al. - 2015 - Going deeper with convolutions.pdf:files/22/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2018-04-09},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {CNN based edge detection.pdf:files/17/CNN based edge detection.pdf:application/pdf;LeCun et al. - 2015 - Deep learning.pdf:files/18/LeCun et al. - 2015 - Deep learning.pdf:application/pdf}
}

@inproceedings{chen_vehicle_2013,
	title = {Vehicle {Detection} in {Satellite} {Images} by {Parallel} {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-2190-4},
	url = {http://ieeexplore.ieee.org/document/6778306/},
	doi = {10.1109/ACPR.2013.33},
	abstract = {Deep convolutional Neural Networks (DNN) is the state-of-the-art machine learning method. It has been used in many recognition tasks including handwritten digits, Chinese words and trafﬁc signs, etc. However, training and test DNN are time-consuming tasks. In practical vehicle detection application, both speed and accuracy are required. So increasing the speeds of DNN while keeping its high accuracy has signiﬁcant meaning for many recognition and detection applications. We introduce parallel branches into the DNN. The maps of the layers of DNN are divided into several parallel branches, each branch has the same number of maps. There are not direct connections between different branches. Our parallel DNN (PNN) keeps the same structure and dimensions of the DNN, reducing the total number of connections between maps. The more number of branches we divide, the more swift the speed of the PNN is, the conventional DNN becomes a special form of PNN which has only one branch. Experiments on large vehicle database showed that the detection accuracy of PNN dropped slightly with the speed increasing. Even the fastest PNN (10 times faster than DNN), whose branch has only two maps, fully outperformed the traditional methods based on features (such as HOG, LBP). In fact, PNN provides a good solution way for compromising the speed and accuracy requirements in many applications.},
	language = {en},
	urldate = {2018-04-09},
	publisher = {IEEE},
	author = {Chen, Xueyun and Xiang, Shiming and Liu, Cheng-Lin and Pan, Chun-Hong},
	month = nov,
	year = {2013},
	pages = {181--185},
	file = {Chen et al. - 2013 - Vehicle Detection in Satellite Images by Parallel .pdf:files/23/Chen et al. - 2013 - Vehicle Detection in Satellite Images by Parallel .pdf:application/pdf}
}

@article{zhong_satcnn:_2017,
	title = {{SatCNN}: satellite image dataset classification using agile convolutional neural networks},
	volume = {8},
	issn = {2150-704X, 2150-7058},
	shorttitle = {{SatCNN}},
	url = {https://www.tandfonline.com/doi/full/10.1080/2150704X.2016.1235299},
	doi = {10.1080/2150704X.2016.1235299},
	abstract = {With the launch of various remote-sensing satellites, more and more high-spatial resolution remote-sensing (HSR-RS) images are becoming available. Scene classiﬁcation of such a huge volume of HSR-RS images is a big challenge for the eﬃciency of the feature learning and model training. The deep convolutional neural network (CNN), a typical deep learning model, is an eﬃcient end-toend deep hierarchical feature learning model that can capture the intrinsic features of input HSR-RS images. However, most published CNN architectures are borrowed from natural scene classiﬁcation with thousands of training samples, and they are not designed for HSR-RS images. In this paper, we propose an agile CNN architecture, named as SatCNN, for HSR-RS image scene classiﬁcation. Based on recent improvements to modern CNN architectures, we use more eﬃcient convolutional layers with smaller kernels to build an eﬀective CNN architecture. Experiments on SAT data sets conﬁrmed that SatCNN can quickly and eﬀectively learn robust features to handle the intra-class diversity even with small convolutional kernels, and the deeper convolutional layers allow spontaneous modelling of the relative spatial relationships. With the help of fast graphics processing unit acceleration, SatCNN can be trained within about 40 min, achieving overall accuracies of 99.65\% and 99.54\%, which is the state-ofthe-art for SAT data sets.},
	language = {en},
	number = {2},
	urldate = {2018-04-09},
	journal = {Remote Sensing Letters},
	author = {Zhong, Yanfei and Fei, Feng and Liu, Yanfei and Zhao, Bei and Jiao, Hongzan and Zhang, Liangpei},
	month = feb,
	year = {2017},
	pages = {136--145},
	file = {Zhong et al. - 2017 - SatCNN satellite image dataset classification usi.pdf:files/21/Zhong et al. - 2017 - SatCNN satellite image dataset classification usi.pdf:application/pdf}
}

@article{zhang_s-cnn-based_2016,
	title = {S-{CNN}-{BASED} {SHIP} {DETECTION} {FROM} {HIGH}-{RESOLUTION} {REMOTE} {SENSING} {IMAGES}},
	volume = {XLI-B7},
	issn = {2194-9034},
	url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B7/423/2016/isprs-archives-XLI-B7-423-2016.pdf},
	doi = {10.5194/isprsarchives-XLI-B7-423-2016},
	abstract = {Reliable ship detection plays an important role in both military and civil ﬁelds. However, it makes the task difﬁcult with high-resolution remote sensing images with complex background and various types of ships with different poses, shapes and scales. Related works mostly used gray and shape features to detect ships, which obtain results with poor robustness and efﬁciency. To detect ships more automatically and robustly, we propose a novel ship detection method based on the convolutional neural networks (CNNs), called SCNN, fed with speciﬁcally designed proposals extracted from the ship model combined with an improved saliency detection method. Firstly we creatively propose two ship models, the “V” ship head model and the “{\textbar}{\textbar}” ship body one, to localize the ship proposals from the line segments extracted from a test image. Next, for offshore ships with relatively small sizes, which cannot be efﬁciently picked out by the ship models due to the lack of reliable line segments, we propose an improved saliency detection method to ﬁnd these proposals. Therefore, these two kinds of ship proposals are fed to the trained CNN for robust and efﬁcient detection. Experimental results on a large amount of representative remote sensing images with different kinds of ships with varied poses, shapes and scales demonstrate the efﬁciency and robustness of our proposed S-CNN-Based ship detector.},
	language = {en},
	urldate = {2018-04-09},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Zhang, Ruiqian and Yao, Jian and Zhang, Kao and Feng, Chen and Zhang, Jiadong},
	month = jun,
	year = {2016},
	pages = {423--430},
	file = {Zhang et al. - 2016 - S-CNN-BASED SHIP DETECTION FROM HIGH-RESOLUTION RE.pdf:files/20/Zhang et al. - 2016 - S-CNN-BASED SHIP DETECTION FROM HIGH-RESOLUTION RE.pdf:application/pdf}
}

@inproceedings{ishii_surface_2015,
	title = {Surface object recognition with {CNN} and {SVM} in {Landsat} 8 images},
	isbn = {978-4-901122-14-6},
	url = {http://ieeexplore.ieee.org/document/7153200/},
	doi = {10.1109/MVA.2015.7153200},
	abstract = {There is a series of earth observation satellites called Landsat, which send a very large amount of image data every day such that it is hard to analyze manually. Thus an eﬀective application of machine learning techniques to automatically analyze such data is called for. In surface object recognition, which is one of the important applications of such data, the distribution of a speciﬁc object on the surface is surveyed. In this paper, we propose and compare two methods for surface object recognition, one using the convolutional neural network (CNN) and the other support vector machine (SVM). In our experiments, CNN showed higher performance than SVM. In addition, we observed that the number of negative samples have a inﬂuence on the performance, and it is necessary to select the number of them for practical use.},
	language = {en},
	urldate = {2018-04-09},
	publisher = {IEEE},
	author = {Ishii, Tomohiro and Nakamura, Ryosuke and Nakada, Hidemoto and Mochizuki, Yoshihiko and Ishikawa, Hiroshi},
	month = may,
	year = {2015},
	pages = {341--344},
	file = {Ishii et al. - 2015 - Surface object recognition with CNN and SVM in Lan.pdf:files/19/Ishii et al. - 2015 - Surface object recognition with CNN and SVM in Lan.pdf:application/pdf}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2018-04-23},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:files/39/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@inproceedings{xie_aggregated_2017,
	title = {Aggregated residual transformations for deep neural networks},
	isbn = {1-5386-0457-4},
	publisher = {IEEE},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	year = {2017},
	pages = {5987--5995}
}
